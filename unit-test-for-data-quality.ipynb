{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data quality and implementing testing strategies for data pipelines in pyhton. code snippet exmaple","metadata":{}},{"cell_type":"code","source":"import unittest\nimport pandas as pd\nimport seaborn as sns\n\n# Your data pipeline functions\ndef load_data():\n    # Load a built-in dataset from Seaborn\n    return sns.load_dataset('iris')\n\ndef transform_data(data):\n    # Sample transformation: doubling the values in a column\n    data['sepal_length'] = data['sepal_length'] * 2\n    return data\n\ndef save_data(data, output_file):\n    data.to_csv(output_file, index=False)\n\n# Test class\nclass TestDataPipeline(unittest.TestCase):\n    \n    def setUp(self):\n        # Set up paths for input and output files\n        self.output_file = 'test_output.csv'\n    \n    def tearDown(self):\n        # Clean up - delete test output file\n        import os\n        os.remove(self.output_file)\n    \n    def test_data_loading(self):\n        # Test if data is loaded correctly\n        loaded_data = load_data()\n        self.assertIsInstance(loaded_data, pd.DataFrame)\n    \n    def test_data_transformation(self):\n        # Test if data is transformed correctly\n        loaded_data = load_data()\n        transformed_data = transform_data(loaded_data.copy())\n        self.assertTrue((transformed_data['sepal_length'] == loaded_data['sepal_length'] * 2).all())\n    \n    def test_data_saving(self):\n        # Test if data is saved correctly\n        loaded_data = load_data()\n        save_data(loaded_data, self.output_file)\n        saved_data = pd.read_csv(self.output_file)\n        self.assertTrue((saved_data['sepal_length'] == loaded_data['sepal_length'] * 2).all())\n\nif __name__ == '__main__':\n    unittest.main()\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}